{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> DeepLearn18 <br>\n",
    "<b> Students:</b> Anna-Maria Turca, Mohammed Saeed\n",
    "  \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288414950\n",
      "Epoch:  02   =====> Loss= 0.732894853\n",
      "Epoch:  03   =====> Loss= 0.600243323\n",
      "Epoch:  04   =====> Loss= 0.536592696\n",
      "Epoch:  05   =====> Loss= 0.497837072\n",
      "Epoch:  06   =====> Loss= 0.470999019\n",
      "Epoch:  07   =====> Loss= 0.451422906\n",
      "Epoch:  08   =====> Loss= 0.435888112\n",
      "Epoch:  09   =====> Loss= 0.423260373\n",
      "Epoch:  10   =====> Loss= 0.413190886\n",
      "Epoch:  11   =====> Loss= 0.404280690\n",
      "Epoch:  12   =====> Loss= 0.397043538\n",
      "Epoch:  13   =====> Loss= 0.390234074\n",
      "Epoch:  14   =====> Loss= 0.384228658\n",
      "Epoch:  15   =====> Loss= 0.379303667\n",
      "Epoch:  16   =====> Loss= 0.374647766\n",
      "Epoch:  17   =====> Loss= 0.370449246\n",
      "Epoch:  18   =====> Loss= 0.366696936\n",
      "Epoch:  19   =====> Loss= 0.362979843\n",
      "Epoch:  20   =====> Loss= 0.359835466\n",
      "Epoch:  21   =====> Loss= 0.356841121\n",
      "Epoch:  22   =====> Loss= 0.353920617\n",
      "Epoch:  23   =====> Loss= 0.351358843\n",
      "Epoch:  24   =====> Loss= 0.348678422\n",
      "Epoch:  25   =====> Loss= 0.346180339\n",
      "Epoch:  26   =====> Loss= 0.344102740\n",
      "Epoch:  27   =====> Loss= 0.342306206\n",
      "Epoch:  28   =====> Loss= 0.340338605\n",
      "Epoch:  29   =====> Loss= 0.338540947\n",
      "Epoch:  30   =====> Loss= 0.336712597\n",
      "Epoch:  31   =====> Loss= 0.335102451\n",
      "Epoch:  32   =====> Loss= 0.333481771\n",
      "Epoch:  33   =====> Loss= 0.331953196\n",
      "Epoch:  34   =====> Loss= 0.330535773\n",
      "Epoch:  35   =====> Loss= 0.329388611\n",
      "Epoch:  36   =====> Loss= 0.327657810\n",
      "Epoch:  37   =====> Loss= 0.326426928\n",
      "Epoch:  38   =====> Loss= 0.325457584\n",
      "Epoch:  39   =====> Loss= 0.324253337\n",
      "Epoch:  40   =====> Loss= 0.323093257\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9164\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "For the <b>first layer</b>, we use a stride of 1 for the <b>Convolution</b> layer to get the required output size. <br> And for the <b>MaxPooling</b> layer, we use a stride of 2 and a windows size of 2 to get the desired output.($$(28-F)/S=14 gives a correct solution for S=F=2)\n",
    "<br>\n",
    "<br>Same thing applies for the second convolution layer.<br>\n",
    "(10-F)/S=5 is solved for S=F=2.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "We now write a function to flatten the output of layer 2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    x_shape=x.get_shape()\n",
    "    num_features=(x_shape[1]*x_shape[2]*x_shape[3])\n",
    "    return tf.reshape(x,[-1,num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "We define variables to be used for the LeNet architecture.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels=1\n",
    "filter_size=5\n",
    "num_filters_1=6\n",
    "num_filters_2=16\n",
    "\n",
    "fc_1=120\n",
    "fc_2=84\n",
    "num_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LeNet5_Model(image):\n",
    "    \n",
    "    #reshape\n",
    "    x_image = tf.reshape(image,[-1,28,28,1])\n",
    "\n",
    "    \n",
    "    \n",
    "    #Layer1\n",
    "    conv1_w=weight_variable(shape=[filter_size,filter_size,num_channels,num_filters_1])\n",
    "    conv1_b=bias_variable([num_filters_1])\n",
    "    conv1=tf.nn.conv2d(x_image,conv1_w,strides=[1,1,1,1],padding=\"SAME\")+conv1_b\n",
    "    #print(conv1)\n",
    "    conv1=tf.nn.relu(conv1)\n",
    "    pool_1=tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    #print(pool_1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Layer2\n",
    "    conv2_w=weight_variable(shape=[filter_size,filter_size,num_filters_1,num_filters_2])\n",
    "    conv2_b=bias_variable([num_filters_2])\n",
    "    conv2=tf.nn.conv2d(pool_1,conv2_w,strides=[1,1,1,1],padding=\"VALID\")+conv2_b\n",
    "    #print(conv2)\n",
    "    conv2=tf.nn.relu(conv2)\n",
    "    pool_2=tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    #print(pool_2)\n",
    "    \n",
    "    #Flatten\n",
    "    flattened=flatten(pool_2)\n",
    "    #print(flattened)\n",
    "    \n",
    "    \n",
    "    #Layer3\n",
    "    fc1_w=weight_variable(shape=[400,120])\n",
    "    fc1_b=bias_variable([120])\n",
    "    fc1=tf.matmul(flattened,fc1_w)+fc1_b\n",
    "    \n",
    "    fc1=tf.nn.relu(fc1)\n",
    "    \n",
    "    \n",
    "    #Layer4\n",
    "    fc2_w=weight_variable(shape=[120,84])\n",
    "    fc2_b=bias_variable([84])\n",
    "    fc2=tf.matmul(fc1,fc2_w)+fc2_b\n",
    "    \n",
    "    fc2=tf.nn.relu(fc2)\n",
    "    \n",
    "    \n",
    "    #Layer5\n",
    "    fc3_w=weight_variable(shape=[84,10])\n",
    "    fc3_b=bias_variable([10])\n",
    "    logits=tf.matmul(fc2,fc3_w)+fc3_b\n",
    "    \n",
    "    return tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "The number of parameters of each layer is the number of elements of the filter multiplied by the number of inputs plus the number of biases.<br>\n",
    "<strong>Number of parameters of Layer 1</strong>: 5x5x1x6 +6=<strong>156</strong><br>\n",
    "<strong>Number of parameters of Layer 2</strong>: 5x5x1x16+16=<strong>416</strong><br>\n",
    "<strong>Number of parameters of Layer 3</strong>: 400x120+120=<strong>48120</strong><br>\n",
    "<strong>Number of parameters of Layer 4</strong>: 120x84+84=<strong>10164</strong><br>\n",
    "<strong>Number of parameters of Layer 5</strong>: 84x10+10=<strong>850</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "The total number of parameters is <b>59706</b> parameters.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    pred=LeNet5_Model(x)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(logits, labels):\n",
    "    # logits will be the outputs of your model, labels will be one-hot vectors corresponding to the actual labels\n",
    "    # logits and labels are numpy arrays\n",
    "    # this function should return the accuracy of your model\n",
    "    \n",
    "    y_pred_classes=tf.argmax(logits,axis=1)\n",
    "    y_true_classes=tf.argmax(labels,axis=1)\n",
    "    \n",
    "    correct_prediction = tf.equal(y_pred_classes, y_true_classes)\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    acc = evaluate(pred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy  tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 2.299616704   Train Accuracy= 0.159072727   Validation Accuracy= 0.156800002\n",
      "Epoch:  02   =====> Loss= 2.281474473   Train Accuracy= 0.231054544   Validation Accuracy= 0.232600003\n",
      "Epoch:  03   =====> Loss= 2.253267493   Train Accuracy= 0.343909085   Validation Accuracy= 0.341399997\n",
      "Epoch:  04   =====> Loss= 2.194353607   Train Accuracy= 0.415781826   Validation Accuracy= 0.408800006\n",
      "Epoch:  05   =====> Loss= 2.051322230   Train Accuracy= 0.488127261   Validation Accuracy= 0.481000006\n",
      "Epoch:  06   =====> Loss= 1.709595099   Train Accuracy= 0.685872734   Validation Accuracy= 0.686800003\n",
      "Epoch:  07   =====> Loss= 1.177147973   Train Accuracy= 0.778781831   Validation Accuracy= 0.784200013\n",
      "Epoch:  08   =====> Loss= 0.781942644   Train Accuracy= 0.829690933   Validation Accuracy= 0.838199973\n",
      "Epoch:  09   =====> Loss= 0.587153607   Train Accuracy= 0.857127249   Validation Accuracy= 0.865599990\n",
      "Epoch:  10   =====> Loss= 0.488564186   Train Accuracy= 0.872854531   Validation Accuracy= 0.880800009\n",
      "Epoch:  11   =====> Loss= 0.431065108   Train Accuracy= 0.883472741   Validation Accuracy= 0.892400026\n",
      "Epoch:  12   =====> Loss= 0.392891466   Train Accuracy= 0.891090930   Validation Accuracy= 0.898800015\n",
      "Epoch:  13   =====> Loss= 0.364830407   Train Accuracy= 0.897472739   Validation Accuracy= 0.904399991\n",
      "Epoch:  14   =====> Loss= 0.342782942   Train Accuracy= 0.902854562   Validation Accuracy= 0.908999979\n",
      "Epoch:  15   =====> Loss= 0.324517836   Train Accuracy= 0.906763613   Validation Accuracy= 0.912599981\n",
      "Epoch:  16   =====> Loss= 0.309438567   Train Accuracy= 0.910600007   Validation Accuracy= 0.915799975\n",
      "Epoch:  17   =====> Loss= 0.296124244   Train Accuracy= 0.914145470   Validation Accuracy= 0.917599976\n",
      "Epoch:  18   =====> Loss= 0.284366591   Train Accuracy= 0.917145431   Validation Accuracy= 0.920000017\n",
      "Epoch:  19   =====> Loss= 0.273857886   Train Accuracy= 0.919472754   Validation Accuracy= 0.923600018\n",
      "Epoch:  20   =====> Loss= 0.264681613   Train Accuracy= 0.922272742   Validation Accuracy= 0.927999973\n",
      "Epoch:  21   =====> Loss= 0.255724799   Train Accuracy= 0.925054550   Validation Accuracy= 0.930599988\n",
      "Epoch:  22   =====> Loss= 0.247511768   Train Accuracy= 0.927436352   Validation Accuracy= 0.933399975\n",
      "Epoch:  23   =====> Loss= 0.239999972   Train Accuracy= 0.929781795   Validation Accuracy= 0.935800016\n",
      "Epoch:  24   =====> Loss= 0.232572078   Train Accuracy= 0.931618154   Validation Accuracy= 0.938000023\n",
      "Epoch:  25   =====> Loss= 0.226101782   Train Accuracy= 0.934054554   Validation Accuracy= 0.939800024\n",
      "Epoch:  26   =====> Loss= 0.220011158   Train Accuracy= 0.935400009   Validation Accuracy= 0.940400004\n",
      "Epoch:  27   =====> Loss= 0.213873593   Train Accuracy= 0.937345445   Validation Accuracy= 0.942200005\n",
      "Epoch:  28   =====> Loss= 0.208155167   Train Accuracy= 0.939145446   Validation Accuracy= 0.943199992\n",
      "Epoch:  29   =====> Loss= 0.202843463   Train Accuracy= 0.940327287   Validation Accuracy= 0.942799985\n",
      "Epoch:  30   =====> Loss= 0.198033001   Train Accuracy= 0.941145480   Validation Accuracy= 0.945400000\n",
      "Epoch:  31   =====> Loss= 0.192850110   Train Accuracy= 0.942981839   Validation Accuracy= 0.945599973\n",
      "Epoch:  32   =====> Loss= 0.188686387   Train Accuracy= 0.943836391   Validation Accuracy= 0.947600007\n",
      "Epoch:  33   =====> Loss= 0.184070203   Train Accuracy= 0.945290923   Validation Accuracy= 0.949199975\n",
      "Epoch:  34   =====> Loss= 0.180131960   Train Accuracy= 0.945890903   Validation Accuracy= 0.950200021\n",
      "Epoch:  35   =====> Loss= 0.176383941   Train Accuracy= 0.947636366   Validation Accuracy= 0.950999975\n",
      "Epoch:  36   =====> Loss= 0.172768789   Train Accuracy= 0.948890924   Validation Accuracy= 0.952600002\n",
      "Epoch:  37   =====> Loss= 0.168992579   Train Accuracy= 0.950345457   Validation Accuracy= 0.954800010\n",
      "Epoch:  38   =====> Loss= 0.165736037   Train Accuracy= 0.951163650   Validation Accuracy= 0.955600023\n",
      "Epoch:  39   =====> Loss= 0.162557307   Train Accuracy= 0.951727271   Validation Accuracy= 0.956600010\n",
      "Epoch:  40   =====> Loss= 0.159218582   Train Accuracy= 0.953090906   Validation Accuracy= 0.957199991\n",
      "Training Finished!\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Final Accuracy: 0.9557\n",
      "Validation Accuracy did not reach 99 percent.\n",
      "Elapsed Time:  1647.5171151161194 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_epochs=40\n",
    "display_step=1\n",
    "n=-1\n",
    "saving_path = 'models/'\n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    saver=tf.train.Saver()\n",
    "    # Training cycle\n",
    "    start_time=time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        train_acc=acc.eval(feed_dict={x: mnist.train.images, y: mnist.train.labels})\n",
    "        val_acc=acc.eval(feed_dict={x: mnist.validation.images,y: mnist.validation.labels})\n",
    "        test_acc=acc.eval(feed_dict={x: mnist.test.images,y: mnist.test.labels})\n",
    "\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost),\n",
    "                  \"  Train Accuracy=\", \"{:.9f}\".format(train_acc),\n",
    "                  \"  Validation Accuracy=\", \"{:.9f}\".format(val_acc))\n",
    "        if (test_acc>0.99):\n",
    "            n=epoch+1\n",
    "            break\n",
    "    end_time=time.time()\n",
    "    time_taken=end_time-start_time\n",
    "    print(\"Training Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Final Accuracy:\", test_acc)\n",
    "    if(n!=-1):\n",
    "        print(\"Test Accuracy over 99 percent reached after %d epochs\" %(n))\n",
    "    else:\n",
    "        print(\"Test Accuracy did not reach 99 percent.\")\n",
    "\n",
    "    save_path = saver.save(sess, saving_path+\"/model\"+str(int(time.time())))\n",
    "\n",
    "    print(\"Elapsed Time: \",time_taken,\"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"MNIST_figures/Accuracy.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 2: Accuracy </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"MNIST_figures/Loss.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 3: Loss </span></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |      95.57%        |        99 .03%      |       \n",
    "| Training Time        |      27.45 min     |        12.47 min    |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "**We can see that the AdamOptimiser gave better results than SGD. We managed to reach an accuracy around 99% on the test set with AdamOptimiser which we didn't reach with SGD and with a much less time. The reason why ADAM is better is that it  calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,learning_rate=0.001,training_epochs=40,batch_size=128,logs_path='log_files/',saving_path='models/',optimiser=\"Adam\",display_step=1):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "    with tf.name_scope('Model'):\n",
    "        pred=model(x)\n",
    "\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "    \n",
    "    \n",
    "    if(optimiser==\"Adam\"):\n",
    "        with tf.name_scope('AdamOptimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    else:\n",
    "        with tf.name_scope('SGD'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        acc = evaluate(pred,y)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy  tensor\n",
    "    tf.summary.scalar(\"Accuracy\", acc)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()    \n",
    "\n",
    "\n",
    "    n=-1\n",
    "\n",
    "\n",
    "    # Launch the graph for training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path+'ADAM', graph=tf.get_default_graph())\n",
    "        saver=tf.train.Saver()\n",
    "\n",
    "        # Training cycle\n",
    "        start_time=time.time()\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            train_acc=acc.eval(feed_dict={x: mnist.train.images,y: mnist.train.labels})\n",
    "            val_acc=acc.eval(feed_dict={x: mnist.validation.images,y: mnist.validation.labels})\n",
    "            test_acc=acc.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost),\n",
    "                \"  Validation Accuracy=\", \"{:.9f}\".format(val_acc), \"  Test Accuracy=\", \"{:.9f}\".format(test_acc))\n",
    "            if (test_acc>0.99):\n",
    "                n=epoch+1\n",
    "                break\n",
    "\n",
    "        end_time=time.time()\n",
    "        time_taken=end_time-start_time\n",
    "        print(\"Training Finished!\")\n",
    "        summary_writer.flush()\n",
    "\n",
    "        # Test model\n",
    "        # Calculate accuracy\n",
    "        print(\"-------------------------------------------------------------------------------------------\")\n",
    "        if(n!=-1):\n",
    "            print(\"Test Accuracy over 99 percent reached after %d epochs\" %(n))\n",
    "        else:\n",
    "            print(\"Test Accuracy did not reach 99 percent.\")        \n",
    "        print(\"Final Accuracy:\", test_acc)\n",
    "        save_path = saver.save(sess, saving_path+\"model_adam\"+str(int(time.time())))\n",
    "\n",
    "        print(\"Elapsed Time: \",time_taken,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 0.342669716   Validation Accuracy= 0.966799974   Test Accuracy= 0.968200028\n",
      "Epoch:  02   =====> Loss= 0.088865482   Validation Accuracy= 0.983399987   Test Accuracy= 0.982500017\n",
      "Epoch:  03   =====> Loss= 0.062367673   Validation Accuracy= 0.984799981   Test Accuracy= 0.985199988\n",
      "Epoch:  04   =====> Loss= 0.047671252   Validation Accuracy= 0.986400008   Test Accuracy= 0.987299979\n",
      "Epoch:  05   =====> Loss= 0.038379036   Validation Accuracy= 0.987200022   Test Accuracy= 0.988600016\n",
      "Epoch:  06   =====> Loss= 0.031156608   Validation Accuracy= 0.986999989   Test Accuracy= 0.988799989\n",
      "Epoch:  07   =====> Loss= 0.026187367   Validation Accuracy= 0.988399982   Test Accuracy= 0.988399982\n",
      "Epoch:  08   =====> Loss= 0.022499809   Validation Accuracy= 0.987999976   Test Accuracy= 0.988099992\n",
      "Epoch:  09   =====> Loss= 0.019960781   Validation Accuracy= 0.987200022   Test Accuracy= 0.987699986\n",
      "Epoch:  10   =====> Loss= 0.017117410   Validation Accuracy= 0.983799994   Test Accuracy= 0.983500004\n",
      "Epoch:  11   =====> Loss= 0.014713628   Validation Accuracy= 0.982599974   Test Accuracy= 0.984300017\n",
      "Epoch:  12   =====> Loss= 0.014456451   Validation Accuracy= 0.981199980   Test Accuracy= 0.982400000\n",
      "Epoch:  13   =====> Loss= 0.012867968   Validation Accuracy= 0.981999993   Test Accuracy= 0.984000027\n",
      "Epoch:  14   =====> Loss= 0.011104819   Validation Accuracy= 0.980199993   Test Accuracy= 0.979099989\n",
      "Epoch:  15   =====> Loss= 0.008836935   Validation Accuracy= 0.986000001   Test Accuracy= 0.985199988\n",
      "Epoch:  16   =====> Loss= 0.009979758   Validation Accuracy= 0.988200009   Test Accuracy= 0.989099979\n",
      "Epoch:  17   =====> Loss= 0.007698398   Validation Accuracy= 0.989000022   Test Accuracy= 0.990300000\n",
      "Training Finished!\n",
      "-------------------------------------------------------------------------------------------\n",
      "Test Accuracy over 99 percent reached after 17 epochs\n",
      "Final Accuracy: 0.9903\n",
      "Elapsed Time:  748.6814961433411 seconds.\n"
     ]
    }
   ],
   "source": [
    "train(LeNet5_Model,learning_rate=0.001,training_epochs=40,batch_size=128,logs_path='log_files/',saving_path='models/',optimiser=\"Adam\",display_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** 98.88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout(image,keep_prob=0.75):    \n",
    "\n",
    "    #reshape\n",
    "    x_image = tf.reshape(image,[-1,28,28,1])\n",
    "\n",
    "    \n",
    "    \n",
    "    #Layer1\n",
    "    conv1_w=weight_variable(shape=[filter_size,filter_size,num_channels,num_filters_1])\n",
    "    conv1_b=bias_variable([num_filters_1])\n",
    "    conv1=tf.nn.conv2d(x_image,conv1_w,strides=[1,1,1,1],padding=\"SAME\")+conv1_b\n",
    "    #print(conv1)\n",
    "    conv1=tf.nn.relu(conv1)\n",
    "    pool_1=tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    \n",
    "    #print(pool_1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Layer2\n",
    "    conv2_w=weight_variable(shape=[filter_size,filter_size,num_filters_1,num_filters_2])\n",
    "    conv2_b=bias_variable([num_filters_2])\n",
    "    conv2=tf.nn.conv2d(pool_1,conv2_w,strides=[1,1,1,1],padding=\"VALID\")+conv2_b\n",
    "    #print(conv2)\n",
    "    conv2=tf.nn.relu(conv2)\n",
    "    pool_2=tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    #print(pool_2)\n",
    "    \n",
    "    #Flatten\n",
    "    flattened=flatten(pool_2)\n",
    "    #print(flattened)\n",
    "    \n",
    "    \n",
    "    #Layer3\n",
    "    fc1_w=weight_variable(shape=[400,120])\n",
    "    fc1_b=bias_variable([120])\n",
    "    fc1=tf.matmul(flattened,fc1_w)+fc1_b\n",
    "    \n",
    "    fc1=tf.nn.relu(fc1)\n",
    "    fc1_drop=tf.nn.dropout(fc1,keep_prob)\n",
    "    \n",
    "    #Layer4\n",
    "    fc2_w=weight_variable(shape=[120,84])\n",
    "    fc2_b=bias_variable([84])\n",
    "    fc2=tf.matmul(fc1_drop,fc2_w)+fc2_b\n",
    "    \n",
    "    fc2=tf.nn.relu(fc2)\n",
    "    \n",
    "    #fc2_drop=tf.nn.dropout(fc2,keep_prob)\n",
    "    \n",
    "    #Layer5\n",
    "    fc3_w=weight_variable(shape=[84,10])\n",
    "    fc3_b=bias_variable([10])\n",
    "    logits=tf.matmul(fc2,fc3_w)+fc3_b\n",
    "    \n",
    "    return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 0.391572584   Validation Accuracy= 0.963199973   Test Accuracy= 0.960099995\n",
      "Epoch:  02   =====> Loss= 0.112643845   Validation Accuracy= 0.973800004   Test Accuracy= 0.975099981\n",
      "Epoch:  03   =====> Loss= 0.083854699   Validation Accuracy= 0.978999972   Test Accuracy= 0.978900015\n",
      "Epoch:  04   =====> Loss= 0.065895880   Validation Accuracy= 0.980000019   Test Accuracy= 0.982599974\n",
      "Epoch:  05   =====> Loss= 0.056328771   Validation Accuracy= 0.983399987   Test Accuracy= 0.983200014\n",
      "Epoch:  06   =====> Loss= 0.048526168   Validation Accuracy= 0.983600020   Test Accuracy= 0.983299971\n",
      "Epoch:  07   =====> Loss= 0.042504138   Validation Accuracy= 0.985000014   Test Accuracy= 0.984600008\n",
      "Epoch:  08   =====> Loss= 0.036820045   Validation Accuracy= 0.985000014   Test Accuracy= 0.984600008\n",
      "Epoch:  09   =====> Loss= 0.035398948   Validation Accuracy= 0.986400008   Test Accuracy= 0.986699998\n",
      "Epoch:  10   =====> Loss= 0.031483554   Validation Accuracy= 0.986000001   Test Accuracy= 0.985899985\n",
      "Epoch:  11   =====> Loss= 0.028372648   Validation Accuracy= 0.986400008   Test Accuracy= 0.987200022\n",
      "Epoch:  12   =====> Loss= 0.026227807   Validation Accuracy= 0.986199975   Test Accuracy= 0.986800015\n",
      "Epoch:  13   =====> Loss= 0.024285938   Validation Accuracy= 0.987200022   Test Accuracy= 0.986199975\n",
      "Epoch:  14   =====> Loss= 0.022906424   Validation Accuracy= 0.987200022   Test Accuracy= 0.989700019\n",
      "Epoch:  15   =====> Loss= 0.021149150   Validation Accuracy= 0.982999980   Test Accuracy= 0.984300017\n",
      "Epoch:  16   =====> Loss= 0.019899378   Validation Accuracy= 0.987600029   Test Accuracy= 0.987299979\n",
      "Epoch:  17   =====> Loss= 0.018145964   Validation Accuracy= 0.987600029   Test Accuracy= 0.987299979\n",
      "Epoch:  18   =====> Loss= 0.017941423   Validation Accuracy= 0.988799989   Test Accuracy= 0.987200022\n",
      "Epoch:  19   =====> Loss= 0.016709618   Validation Accuracy= 0.986000001   Test Accuracy= 0.989300013\n",
      "Epoch:  20   =====> Loss= 0.016019197   Validation Accuracy= 0.989799976   Test Accuracy= 0.990000010\n",
      "Training Finished!\n",
      "-------------------------------------------------------------------------------------------\n",
      "Test Accuracy over 99 percent reached after 20 epochs\n",
      "Final Accuracy: 0.99\n",
      "Elapsed Time:  427.3297510147095 seconds.\n"
     ]
    }
   ],
   "source": [
    "train(LeNet5_Model_Dropout,learning_rate=0.001,training_epochs=40,batch_size=128,logs_path='log_files/ADAM_DROPOUT',saving_path='models/ADAM_DROPOUT',optimiser=\"Adam\",display_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Adding dropout allowed us to reach 99% accuracy on the test data with only 7 minutes compared to 12 minutes without dropout.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Let's try now adding dropout to other layers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout2(image,keep_prob=0.75):    \n",
    "\n",
    "    #reshape\n",
    "    x_image = tf.reshape(image,[-1,28,28,1])\n",
    "\n",
    "    \n",
    "    \n",
    "    #Layer1\n",
    "    conv1_w=weight_variable(shape=[filter_size,filter_size,num_channels,num_filters_1])\n",
    "    conv1_b=bias_variable([num_filters_1])\n",
    "    conv1=tf.nn.conv2d(x_image,conv1_w,strides=[1,1,1,1],padding=\"SAME\")+conv1_b\n",
    "    #print(conv1)\n",
    "    conv1=tf.nn.relu(conv1)\n",
    "    pool_1=tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    \n",
    "    #print(pool_1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Layer2\n",
    "    conv2_w=weight_variable(shape=[filter_size,filter_size,num_filters_1,num_filters_2])\n",
    "    conv2_b=bias_variable([num_filters_2])\n",
    "    conv2=tf.nn.conv2d(pool_1,conv2_w,strides=[1,1,1,1],padding=\"VALID\")+conv2_b\n",
    "    #print(conv2)\n",
    "    conv2=tf.nn.relu(conv2)\n",
    "    pool_2=tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    #print(pool_2)\n",
    "    \n",
    "    #Flatten\n",
    "    flattened=flatten(pool_2)\n",
    "    #print(flattened)\n",
    "    \n",
    "    \n",
    "    #Layer3\n",
    "    fc1_w=weight_variable(shape=[400,120])\n",
    "    fc1_b=bias_variable([120])\n",
    "    fc1=tf.matmul(flattened,fc1_w)+fc1_b\n",
    "    \n",
    "    fc1=tf.nn.relu(fc1)\n",
    "    fc1_drop=tf.nn.dropout(fc1,keep_prob)\n",
    "    \n",
    "    #Layer4\n",
    "    fc2_w=weight_variable(shape=[120,84])\n",
    "    fc2_b=bias_variable([84])\n",
    "    fc2=tf.matmul(fc1_drop,fc2_w)+fc2_b\n",
    "    \n",
    "    fc2=tf.nn.relu(fc2)\n",
    "    \n",
    "    fc2_drop=tf.nn.dropout(fc2,keep_prob)\n",
    "    \n",
    "    #Layer5\n",
    "    fc3_w=weight_variable(shape=[84,10])\n",
    "    fc3_b=bias_variable([10])\n",
    "    logits=tf.matmul(fc2_drop,fc3_w)+fc3_b\n",
    "    \n",
    "    return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 0.513664424   Validation Accuracy= 0.953199983   Test Accuracy= 0.944899976\n",
      "Epoch:  02   =====> Loss= 0.149366778   Validation Accuracy= 0.967400014   Test Accuracy= 0.965300024\n",
      "Epoch:  03   =====> Loss= 0.105040194   Validation Accuracy= 0.973999977   Test Accuracy= 0.974699974\n",
      "Epoch:  04   =====> Loss= 0.082503379   Validation Accuracy= 0.979600012   Test Accuracy= 0.978299975\n",
      "Epoch:  05   =====> Loss= 0.071435529   Validation Accuracy= 0.983399987   Test Accuracy= 0.981700003\n",
      "Epoch:  06   =====> Loss= 0.060406025   Validation Accuracy= 0.980799973   Test Accuracy= 0.981599987\n",
      "Epoch:  07   =====> Loss= 0.055404045   Validation Accuracy= 0.983600020   Test Accuracy= 0.981700003\n",
      "Epoch:  08   =====> Loss= 0.047677740   Validation Accuracy= 0.984000027   Test Accuracy= 0.981199980\n",
      "Epoch:  09   =====> Loss= 0.045840917   Validation Accuracy= 0.981400013   Test Accuracy= 0.980700016\n",
      "Epoch:  10   =====> Loss= 0.039982803   Validation Accuracy= 0.982999980   Test Accuracy= 0.983399987\n",
      "Epoch:  11   =====> Loss= 0.036645523   Validation Accuracy= 0.981800020   Test Accuracy= 0.978999972\n",
      "Epoch:  12   =====> Loss= 0.034481644   Validation Accuracy= 0.984399974   Test Accuracy= 0.983500004\n",
      "Epoch:  13   =====> Loss= 0.032571298   Validation Accuracy= 0.983600020   Test Accuracy= 0.981400013\n",
      "Epoch:  14   =====> Loss= 0.029789733   Validation Accuracy= 0.983799994   Test Accuracy= 0.984899998\n",
      "Epoch:  15   =====> Loss= 0.026895172   Validation Accuracy= 0.985800028   Test Accuracy= 0.986000001\n",
      "Epoch:  16   =====> Loss= 0.027590040   Validation Accuracy= 0.986599982   Test Accuracy= 0.984799981\n",
      "Epoch:  17   =====> Loss= 0.027464553   Validation Accuracy= 0.987399995   Test Accuracy= 0.988200009\n",
      "Epoch:  18   =====> Loss= 0.023382780   Validation Accuracy= 0.987800002   Test Accuracy= 0.986599982\n",
      "Epoch:  19   =====> Loss= 0.021539047   Validation Accuracy= 0.987600029   Test Accuracy= 0.988799989\n",
      "Epoch:  20   =====> Loss= 0.020974899   Validation Accuracy= 0.987800002   Test Accuracy= 0.986800015\n",
      "Epoch:  21   =====> Loss= 0.019838621   Validation Accuracy= 0.988200009   Test Accuracy= 0.986999989\n",
      "Epoch:  22   =====> Loss= 0.018923014   Validation Accuracy= 0.987600029   Test Accuracy= 0.987600029\n",
      "Epoch:  23   =====> Loss= 0.018342661   Validation Accuracy= 0.987600029   Test Accuracy= 0.986400008\n",
      "Epoch:  24   =====> Loss= 0.016925702   Validation Accuracy= 0.990999997   Test Accuracy= 0.987100005\n",
      "Epoch:  25   =====> Loss= 0.014936382   Validation Accuracy= 0.987999976   Test Accuracy= 0.986800015\n",
      "Epoch:  26   =====> Loss= 0.016183582   Validation Accuracy= 0.988399982   Test Accuracy= 0.988499999\n",
      "Epoch:  27   =====> Loss= 0.015677033   Validation Accuracy= 0.989000022   Test Accuracy= 0.987500012\n",
      "Epoch:  28   =====> Loss= 0.014795848   Validation Accuracy= 0.989799976   Test Accuracy= 0.987399995\n",
      "Epoch:  29   =====> Loss= 0.013534259   Validation Accuracy= 0.989199996   Test Accuracy= 0.988600016\n",
      "Epoch:  30   =====> Loss= 0.012489464   Validation Accuracy= 0.987999976   Test Accuracy= 0.988699973\n",
      "Epoch:  31   =====> Loss= 0.014487640   Validation Accuracy= 0.989600003   Test Accuracy= 0.986000001\n",
      "Epoch:  32   =====> Loss= 0.012974969   Validation Accuracy= 0.989199996   Test Accuracy= 0.986999989\n",
      "Epoch:  33   =====> Loss= 0.012190532   Validation Accuracy= 0.987800002   Test Accuracy= 0.987200022\n",
      "Epoch:  34   =====> Loss= 0.014083937   Validation Accuracy= 0.986999989   Test Accuracy= 0.987800002\n",
      "Epoch:  35   =====> Loss= 0.012310131   Validation Accuracy= 0.988200009   Test Accuracy= 0.989700019\n",
      "Epoch:  36   =====> Loss= 0.010851347   Validation Accuracy= 0.989600003   Test Accuracy= 0.987699986\n",
      "Epoch:  37   =====> Loss= 0.011684535   Validation Accuracy= 0.988600016   Test Accuracy= 0.987500012\n",
      "Epoch:  38   =====> Loss= 0.011197491   Validation Accuracy= 0.987600029   Test Accuracy= 0.986899972\n",
      "Epoch:  39   =====> Loss= 0.012483524   Validation Accuracy= 0.989600003   Test Accuracy= 0.988099992\n",
      "Epoch:  40   =====> Loss= 0.009467178   Validation Accuracy= 0.990599990   Test Accuracy= 0.986800015\n",
      "Training Finished!\n",
      "-------------------------------------------------------------------------------------------\n",
      "Test Accuracy did not reach 99 percent.\n",
      "Final Accuracy: 0.9868\n",
      "Elapsed Time:  833.2297487258911 seconds.\n"
     ]
    }
   ],
   "source": [
    "train(LeNet5_Model_Dropout2,learning_rate=0.001,training_epochs=40,batch_size=128,logs_path='log_files/ADAM_DROPOUT2',saving_path='models/ADAM_DROPOUT2',optimiser=\"Adam\",display_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Adding dropout to other layers didn't yield us an accuracy of 99% on the test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Let's try exponentially decreasing the learning rate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,training_epochs=40,batch_size=128,logs_path='log_files/',saving_path='models/',display_step=1):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "    with tf.name_scope('Model'):\n",
    "        pred=model(x)\n",
    "\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "    \n",
    "    \n",
    "        with tf.name_scope('AdamOptimizer'):\n",
    "            \n",
    "            batch = tf.Variable(0)\n",
    "            \n",
    "            train_size=len(X_train)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "            1e-4,  # Base learning rate.\n",
    "            batch * batch_size,  # Current index into the dataset.\n",
    "            train_size,  # Decay step.\n",
    "            0.95,  # Decay rate.\n",
    "            staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.name_scope('Accuracy'):\n",
    "        acc = evaluate(pred,y)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy  tensor\n",
    "    tf.summary.scalar(\"Accuracy\", acc)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Launch the graph for training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path+'ADAM', graph=tf.get_default_graph())\n",
    "        saver=tf.train.Saver()\n",
    "\n",
    "        # Training cycle\n",
    "        start_time=time.time()\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            train_acc=acc.eval(feed_dict={x: mnist.train.images,y: mnist.train.labels})\n",
    "            val_acc=acc.eval(feed_dict={x: mnist.validation.images,y: mnist.validation.labels})\n",
    "            test_acc=acc.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost),\n",
    "                \"  Validation Accuracy=\", \"{:.9f}\".format(val_acc), \"  Test Accuracy=\", \"{:.9f}\".format(test_acc))\n",
    "\n",
    "\n",
    "        end_time=time.time()\n",
    "        time_taken=end_time-start_time\n",
    "        print(\"Training Finished!\")\n",
    "        summary_writer.flush()\n",
    "\n",
    "        # Test model\n",
    "        # Calculate accuracy\n",
    "        print(\"-------------------------------------------------------------------------------------------\")      \n",
    "        print(\"Final Accuracy:\", test_acc)\n",
    "        save_path = saver.save(sess, saving_path+\"model_adam\"+str(int(time.time())))\n",
    "\n",
    "        print(\"Elapsed Time: \",time_taken,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.396593710   Validation Accuracy= 0.821600020   Test Accuracy= 0.819599986\n",
      "Epoch:  02   =====> Loss= 0.446355302   Validation Accuracy= 0.898800015   Test Accuracy= 0.902000010\n",
      "Epoch:  03   =====> Loss= 0.299992197   Validation Accuracy= 0.922800004   Test Accuracy= 0.926299989\n",
      "Epoch:  04   =====> Loss= 0.234137837   Validation Accuracy= 0.934199989   Test Accuracy= 0.938199997\n",
      "Epoch:  05   =====> Loss= 0.198031092   Validation Accuracy= 0.949400008   Test Accuracy= 0.948300004\n",
      "Epoch:  06   =====> Loss= 0.172616107   Validation Accuracy= 0.953199983   Test Accuracy= 0.953700006\n",
      "Epoch:  07   =====> Loss= 0.152028604   Validation Accuracy= 0.962199986   Test Accuracy= 0.957899988\n",
      "Epoch:  08   =====> Loss= 0.137433209   Validation Accuracy= 0.959999979   Test Accuracy= 0.959800005\n",
      "Epoch:  09   =====> Loss= 0.126874997   Validation Accuracy= 0.964600027   Test Accuracy= 0.964999974\n",
      "Epoch:  10   =====> Loss= 0.117239229   Validation Accuracy= 0.971400023   Test Accuracy= 0.967000008\n",
      "Epoch:  11   =====> Loss= 0.110129827   Validation Accuracy= 0.969600022   Test Accuracy= 0.968999982\n",
      "Epoch:  12   =====> Loss= 0.102562668   Validation Accuracy= 0.971400023   Test Accuracy= 0.972500026\n",
      "Epoch:  13   =====> Loss= 0.096806228   Validation Accuracy= 0.973200023   Test Accuracy= 0.971899986\n",
      "Epoch:  14   =====> Loss= 0.092101478   Validation Accuracy= 0.972599983   Test Accuracy= 0.972800016\n",
      "Epoch:  15   =====> Loss= 0.089135098   Validation Accuracy= 0.974399984   Test Accuracy= 0.973100007\n",
      "Epoch:  16   =====> Loss= 0.085563416   Validation Accuracy= 0.978600025   Test Accuracy= 0.973399997\n",
      "Epoch:  17   =====> Loss= 0.082735572   Validation Accuracy= 0.977999985   Test Accuracy= 0.976499975\n",
      "Epoch:  18   =====> Loss= 0.077359094   Validation Accuracy= 0.976999998   Test Accuracy= 0.976899981\n",
      "Epoch:  19   =====> Loss= 0.073843234   Validation Accuracy= 0.977999985   Test Accuracy= 0.978900015\n",
      "Epoch:  20   =====> Loss= 0.070380782   Validation Accuracy= 0.980199993   Test Accuracy= 0.977699995\n",
      "Epoch:  21   =====> Loss= 0.070695682   Validation Accuracy= 0.979799986   Test Accuracy= 0.978799999\n",
      "Epoch:  22   =====> Loss= 0.066686559   Validation Accuracy= 0.980599999   Test Accuracy= 0.979700029\n",
      "Epoch:  23   =====> Loss= 0.065581541   Validation Accuracy= 0.983600020   Test Accuracy= 0.977900028\n",
      "Epoch:  24   =====> Loss= 0.063035561   Validation Accuracy= 0.979799986   Test Accuracy= 0.979600012\n",
      "Epoch:  25   =====> Loss= 0.062042713   Validation Accuracy= 0.983600020   Test Accuracy= 0.978600025\n",
      "Epoch:  26   =====> Loss= 0.058960268   Validation Accuracy= 0.982200027   Test Accuracy= 0.979799986\n",
      "Epoch:  27   =====> Loss= 0.059108989   Validation Accuracy= 0.981400013   Test Accuracy= 0.981400013\n",
      "Epoch:  28   =====> Loss= 0.056341830   Validation Accuracy= 0.983200014   Test Accuracy= 0.982800007\n",
      "Epoch:  29   =====> Loss= 0.055128575   Validation Accuracy= 0.981800020   Test Accuracy= 0.981199980\n",
      "Epoch:  30   =====> Loss= 0.054121544   Validation Accuracy= 0.983600020   Test Accuracy= 0.982200027\n",
      "Epoch:  31   =====> Loss= 0.051691212   Validation Accuracy= 0.983600020   Test Accuracy= 0.981999993\n",
      "Epoch:  32   =====> Loss= 0.050559236   Validation Accuracy= 0.981599987   Test Accuracy= 0.981999993\n",
      "Epoch:  33   =====> Loss= 0.048433441   Validation Accuracy= 0.981999993   Test Accuracy= 0.983699977\n",
      "Epoch:  34   =====> Loss= 0.047732164   Validation Accuracy= 0.982200027   Test Accuracy= 0.981400013\n",
      "Epoch:  35   =====> Loss= 0.049199483   Validation Accuracy= 0.980599999   Test Accuracy= 0.983399987\n",
      "Epoch:  36   =====> Loss= 0.045446304   Validation Accuracy= 0.983600020   Test Accuracy= 0.980899990\n",
      "Epoch:  37   =====> Loss= 0.043579545   Validation Accuracy= 0.984200001   Test Accuracy= 0.984300017\n",
      "Epoch:  38   =====> Loss= 0.042811802   Validation Accuracy= 0.983600020   Test Accuracy= 0.982100010\n",
      "Epoch:  39   =====> Loss= 0.041971026   Validation Accuracy= 0.985199988   Test Accuracy= 0.984099984\n",
      "Epoch:  40   =====> Loss= 0.042375917   Validation Accuracy= 0.984000027   Test Accuracy= 0.982999980\n",
      "Training Finished!\n",
      "-------------------------------------------------------------------------------------------\n",
      "Final Accuracy: 0.983\n",
      "Elapsed Time:  1232.5795350074768 seconds.\n"
     ]
    }
   ],
   "source": [
    "train(LeNet5_Model_Dropout,training_epochs=40,batch_size=128,logs_path='log_files/LeNet2',saving_path='models/LeNet2',display_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "We didn't get the 99% accuracy we wanted. Other things we could do is tune the decay factor and apply batch normalisation the layers.</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
