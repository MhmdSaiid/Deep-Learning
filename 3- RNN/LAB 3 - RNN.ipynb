{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 3 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Sentiment Analysis with Recurrent Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> DeepLearn18 (Mohammed Saeed, Ana-Maria Tarca)\n",
    " \n",
    " \n",
    "The aim of this session is to practice with VanillaRNN and Gated Recurrent Units (GRU). Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report in HTML and upload it (along with any necessary images files using a zip archive) on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by June 15th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Sentiment Analysis with a Vanilla RNN\n",
    "\n",
    "In this part, you have no code to write. However you should spend some time stydying the code provided, to fully understand how the Vanilla RNN is implemented: you will implement a GRU in a similar way in Section 2.\n",
    "\n",
    "You will work on a corpus of 3,000 user comments taken from IMDb (1,000), Amazon (1,000) and Yelp (1,000). These comments are split into two categories: positive comments (denoted by \"1\") and negative comments (denoted by \"0\"). For each website, 500 comments are positive and 500 comments are negative. This corpus has been created for the paper <i>From Group to Individual Labels using Deep Features</i> by Kotzias <i>et al</i> (KDD '15 Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Pages 597-606, Sydney, NSW, Australia — August 10 - 13, 2015, ACM New York, NY, USA ©2015  ISBN: 978-1-4503-3664-2 doi>10.1145/2783258.2783380).\n",
    "\n",
    "In this lab, we split this dataset into a training set of 2,520 comments (420 positive comments and 420 negative comments from each website), a validation set of 240 comments (40 positive comments and 40 negative comments from each website) and a test set of 240 comments (40 positive comments and 40 negative comments from each website).\n",
    "\n",
    "Your goal will be to classify automatically these sentences by training a Vanilla RNN and then a GRU. Please note that we use the word2vec method to convert words into vectors (Embedding of 300 dimensions in this lab): these vectors are designed so that they reflect the semantic and the syntactic functions of words. You can read more about word2vec in the paper <i>Distributed representations of words and phrases and their compositionality</i> by Mikolov <i>et al.</i> (NIPS'13 Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2, Pages 3111-3119, Lake Tahoe, Nevada — December 05 - 10, 2013).\n",
    "\n",
    "First of all, please run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# Parameters\n",
    "epsilon = 1e-10\n",
    "max_l = 32 # Max length of sentences\n",
    "\n",
    "train, val, test, word2vec = utils.load_data()\n",
    "data = utils.Dataset(train, val, test, word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we define a VanillaRNN class. Please read its code carefully before running the cell because you will need to implement a similar class for the GRU.\n",
    "\n",
    "If our sentence is represented by the sequence $(x_1, ..., x_L)$, the hidden states $h_t$ of the Vanilla RNN are defined as\n",
    "\n",
    "<div align=\"center\">$h_0 = 0$</div>\n",
    "<div align=\"center\">$h_{t+1} = f(W_h h_t + W_x x_{t+1} + b)$</div>\n",
    "\n",
    "where $W_h$, $W_x$ and $b$ are trainable parameters and $f$ is an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "\n",
    "    def __init__(self, input_size, hidden_states, activation=None, name=None):\n",
    "        self._hidden_states = hidden_states\n",
    "        self._input_size = input_size\n",
    "        self._activation = activation or tf.tanh\n",
    "        self._name = (name or \"vanilla_rnn\") + \"/\"\n",
    "        self._candidate_kernel = tf.get_variable(self._name + \"candidate/weights\",\n",
    "                                                   shape=[input_size + self._hidden_states, self._hidden_states])\n",
    "        self._candidate_bias = tf.get_variable(self._name + \"candidate/bias\", shape=[self._hidden_states])\n",
    "\n",
    "    def state_size(self):\n",
    "        return self._hidden_states\n",
    "\n",
    "    def zero_state(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        return tf.zeros([batch_size, self.state_size()], dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, inputs, state):\n",
    "\n",
    "        candidate = tf.matmul(tf.concat([inputs, state], 1), self._candidate_kernel)\n",
    "        candidate = tf.nn.bias_add(candidate, self._candidate_bias)\n",
    "        new_h = self._activation(candidate)\n",
    "        return new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Parameters</b>\n",
    "* Learning rate: 0.001\n",
    "* Training epochs: 30\n",
    "* Batch size: 128\n",
    "* Hidden states: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 30\n",
    "batch_size = 128\n",
    "hidden_states = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our model. Please read the code of the process_sequence() function to understand the utility of the MaskData placeholder. If $h_L$ is the last hidden state of the Vanilla RNN, then we define our final prediction $p$ as\n",
    "\n",
    "<div align=\"center\">$p = \\sigma (W_{pred} h_L + b_{pred})$</div>\n",
    "\n",
    "where $W_{pred}$ and $b_{pred}$ are trainable parameters and $\\sigma$ denotes the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(123)\n",
    "model_path = \"models/vanilla.ckpt\"\n",
    "# tf Graph Input:  sentiment analysis data\n",
    "# Sentences are padded with zero vectors\n",
    "x = tf.placeholder(tf.float32, [None, max_l, 300], name='InputData')\n",
    "# masks: necessary as we have different sentence lengths\n",
    "m = tf.placeholder(tf.float32, [None, max_l, 1], name='MaskData')\n",
    "# positive (1) or negative (0) labels\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='LabelData')\n",
    "\n",
    "# we define our VanillaRNN cell\n",
    "vanilla = VanillaRNN(300, hidden_states)\n",
    "\n",
    "# we retrieve its last output\n",
    "vanilla_output = utils.process_sequence(vanilla, x, m)\n",
    "\n",
    "W = tf.Variable(tf.zeros([hidden_states, 1]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([1]), name='Bias')\n",
    "# we make the final prediction\n",
    "pred = tf.nn.sigmoid(tf.matmul(vanilla_output, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we train our model using a cross-entropy loss and the Adam optimizer. At each epoch we check the validation accuracy, and save the model if that accuracy increased. At the end, we load the best model on validation, and print its accuracy on the test set.\n",
    "\n",
    "We test our model using a $\\tanh$ activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Accuracy on validation: 0.5\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  01   =====> Loss= 0.694689286\n",
      "Accuracy on validation: 0.6666667\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  02   =====> Loss= 0.690510929\n",
      "Accuracy on validation: 0.7416667\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  03   =====> Loss= 0.662441818\n",
      "Accuracy on validation: 0.69166666\n",
      "Epoch:  04   =====> Loss= 0.582807055\n",
      "Accuracy on validation: 0.77916664\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  05   =====> Loss= 0.528966646\n",
      "Accuracy on validation: 0.79583335\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  06   =====> Loss= 0.471235694\n",
      "Accuracy on validation: 0.8\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  07   =====> Loss= 0.430951139\n",
      "Accuracy on validation: 0.8208333\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  08   =====> Loss= 0.417721425\n",
      "Accuracy on validation: 0.8208333\n",
      "Epoch:  09   =====> Loss= 0.395720712\n",
      "Accuracy on validation: 0.8208333\n",
      "Epoch:  10   =====> Loss= 0.373930128\n",
      "Accuracy on validation: 0.8208333\n",
      "Epoch:  11   =====> Loss= 0.357442500\n",
      "Accuracy on validation: 0.85\n",
      "        Model saved in file: models/vanilla.ckpt\n",
      "Epoch:  12   =====> Loss= 0.356331010\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  13   =====> Loss= 0.345986431\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  14   =====> Loss= 0.338980397\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  15   =====> Loss= 0.332500713\n",
      "Accuracy on validation: 0.84583336\n",
      "Epoch:  16   =====> Loss= 0.323512348\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  17   =====> Loss= 0.313219003\n",
      "Accuracy on validation: 0.84166664\n",
      "Epoch:  18   =====> Loss= 0.298333220\n",
      "Accuracy on validation: 0.825\n",
      "Epoch:  19   =====> Loss= 0.315214659\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  20   =====> Loss= 0.303807953\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  21   =====> Loss= 0.288268915\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  22   =====> Loss= 0.280946296\n",
      "Accuracy on validation: 0.84166664\n",
      "Epoch:  23   =====> Loss= 0.270558098\n",
      "Accuracy on validation: 0.84166664\n",
      "Epoch:  24   =====> Loss= 0.273010323\n",
      "Accuracy on validation: 0.8333333\n",
      "Epoch:  25   =====> Loss= 0.263132624\n",
      "Accuracy on validation: 0.8208333\n",
      "Epoch:  26   =====> Loss= 0.260307807\n",
      "Accuracy on validation: 0.84583336\n",
      "Epoch:  27   =====> Loss= 0.246748370\n",
      "Accuracy on validation: 0.84166664\n",
      "Epoch:  28   =====> Loss= 0.230553646\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  29   =====> Loss= 0.230917430\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  30   =====> Loss= 0.234823135\n",
      "INFO:tensorflow:Restoring parameters from models/vanilla.ckpt\n",
      "Accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)) - (1.-y)*tf.log(tf.clip_by_value((1.-pred), epsilon, 1.0)))\n",
    "\n",
    "with tf.name_scope('Adam'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    pred_tmp = tf.stack([pred, 1.-pred])\n",
    "    y_tmp = tf.stack([y, 1.-y])\n",
    "    acc = tf.equal(tf.argmax(pred_tmp, 0), tf.argmax(y_tmp, 0))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    print(\"Training started\")\n",
    "    best_val_acc = 0.\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(train)/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ms, batch_ys = data.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c = sess.run([optimizer, cost],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys, m: batch_ms})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        val_xs, val_ms, val_ys = data.val_batch()\n",
    "        val_acc = acc.eval({x: val_xs, m: val_ms, y: val_ys})\n",
    "        print(\"Accuracy on validation:\", val_acc)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = saver.save(sess, model_path)\n",
    "            print(\"        Model saved in file: %s\" % save_path)\n",
    "        print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    saver.restore(sess, model_path)\n",
    "    test_xs, test_ms, test_ys = data.test_batch()\n",
    "    print(\"Accuracy:\", acc.eval({x: test_xs, m: test_ms, y: test_ys}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you understand everything? If so, you can move towards Section 2.\n",
    "\n",
    "# Section 2: Your turn!\n",
    "\n",
    "<b>Question 1</b> - Recall the formulas defining the hidden states of a GRU.\n",
    "\n",
    "\\begin{equation}\n",
    "   \\textbf{Remember}\\\\\n",
    "   r^t = \\sigma(W_r\\cdot[h^{t-1},x^t]+b_r) \\\\\n",
    "   \\textbf{Input}\\\\\n",
    "   h^{\\prime t} = tanh(W_i\\cdot[r^t \\otimes h^{t-1},x^t]+b_i)\\\\\n",
    "   \\textbf{Update}\\\\\n",
    "   z^t = \\sigma(W_z\\cdot[h^{t-1},x^t]+b_z)\\\\\n",
    "   h^t = z^t \\otimes h^{\\prime t} + (1-z^t) \\otimes h^{t-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2</b> - Define a GRU similar to the Vanilla RNN that we defined in Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "\n",
    "    def __init__(self, input_size, hidden_states, activation=None, name=None):\n",
    "        self._hidden_states = hidden_states\n",
    "        self._input_size = input_size\n",
    "        self._activation = activation or tf.tanh\n",
    "        self._name = (name or \"gru\") + \"/\"\n",
    "        ############ CODE NEEDED ############\n",
    "        # Define trainable parameters here  #\n",
    "        #####################################\n",
    "        \n",
    "        \n",
    "        self._input_kernel = tf.get_variable(self._name + \"input/weights\",\n",
    "                                                   shape=[input_size + self._hidden_states, self._hidden_states])\n",
    "        self._input_bias = tf.get_variable(self._name + \"input/bias\", shape=[self._hidden_states])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self._remember_kernel = tf.get_variable(self._name + \"remember/weights\",\n",
    "                                                   shape=[input_size + self._hidden_states, self._hidden_states])\n",
    "        self._remember_bias = tf.get_variable(self._name + \"remember/bias\", shape=[self._hidden_states])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self._update_kernel = tf.get_variable(self._name + \"update/weights\",\n",
    "                                                   shape=[input_size + self._hidden_states, self._hidden_states])\n",
    "        self._update_bias = tf.get_variable(self._name + \"update/bias\", shape=[self._hidden_states])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def state_size(self):\n",
    "        return self._hidden_states\n",
    "\n",
    "    def output_size(self):\n",
    "        return self._hidden_states\n",
    "\n",
    "    def zero_state(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        return tf.zeros([batch_size, self.state_size()], dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, inputs, state):\n",
    "        ############ CODE NEEDED ############\n",
    "        #  Write GRU operations according   #\n",
    "        #   to your answer at question 1    #\n",
    "        #####################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        remember=tf.matmul(tf.concat([state, inputs], 1), self._remember_kernel)\n",
    "        remember=tf.nn.bias_add(remember,self._remember_bias)\n",
    "        remember=tf.nn.sigmoid(remember)\n",
    "        \n",
    "        \n",
    "        \n",
    "        inpt=tf.matmul(tf.concat([tf.multiply(state,remember), inputs], 1), self._input_kernel)\n",
    "        inpt=tf.nn.bias_add(inpt,self._input_bias)\n",
    "        inpt=tf.tanh(inpt)\n",
    "\n",
    "        \n",
    "        \n",
    "        update=tf.matmul(tf.concat([state, inputs], 1), self._update_kernel)\n",
    "        update=tf.nn.bias_add(update,self._update_bias)\n",
    "        update=tf.nn.sigmoid(update)\n",
    "        \n",
    "        \n",
    "        new_h=tf.multiply(update,inpt)+tf.multiply((1-update),state)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return new_h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3</b> - Train that GRU with a $tanh$ activation function and print its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Accuracy on validation: 0.575\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  01   =====> Loss= 0.691214392\n",
      "Accuracy on validation: 0.75\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  02   =====> Loss= 0.663259346\n",
      "Accuracy on validation: 0.775\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  03   =====> Loss= 0.545860372\n",
      "Accuracy on validation: 0.79583335\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  04   =====> Loss= 0.445721775\n",
      "Accuracy on validation: 0.8041667\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  05   =====> Loss= 0.400917941\n",
      "Accuracy on validation: 0.8125\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  06   =====> Loss= 0.380924443\n",
      "Accuracy on validation: 0.825\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  07   =====> Loss= 0.357606259\n",
      "Accuracy on validation: 0.8333333\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  08   =====> Loss= 0.341531901\n",
      "Accuracy on validation: 0.84166664\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  09   =====> Loss= 0.324785248\n",
      "Accuracy on validation: 0.84166664\n",
      "Epoch:  10   =====> Loss= 0.316584802\n",
      "Accuracy on validation: 0.84166664\n",
      "Epoch:  11   =====> Loss= 0.306977161\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  12   =====> Loss= 0.298187472\n",
      "Accuracy on validation: 0.825\n",
      "Epoch:  13   =====> Loss= 0.289324254\n",
      "Accuracy on validation: 0.84583336\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  14   =====> Loss= 0.281758616\n",
      "Accuracy on validation: 0.8625\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  15   =====> Loss= 0.271882533\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  16   =====> Loss= 0.258732466\n",
      "Accuracy on validation: 0.8625\n",
      "Epoch:  17   =====> Loss= 0.247506851\n",
      "Accuracy on validation: 0.84583336\n",
      "Epoch:  18   =====> Loss= 0.245506553\n",
      "Accuracy on validation: 0.85\n",
      "Epoch:  19   =====> Loss= 0.243315586\n",
      "Accuracy on validation: 0.85\n",
      "Epoch:  20   =====> Loss= 0.222721632\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  21   =====> Loss= 0.214339593\n",
      "Accuracy on validation: 0.84583336\n",
      "Epoch:  22   =====> Loss= 0.198306268\n",
      "Accuracy on validation: 0.85833335\n",
      "Epoch:  23   =====> Loss= 0.196903528\n",
      "Accuracy on validation: 0.84583336\n",
      "Epoch:  24   =====> Loss= 0.187009014\n",
      "Accuracy on validation: 0.8333333\n",
      "Epoch:  25   =====> Loss= 0.179267434\n",
      "Accuracy on validation: 0.8333333\n",
      "Epoch:  26   =====> Loss= 0.172505162\n",
      "Accuracy on validation: 0.8333333\n",
      "Epoch:  27   =====> Loss= 0.158121696\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  28   =====> Loss= 0.144414390\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  29   =====> Loss= 0.136095583\n",
      "Accuracy on validation: 0.825\n",
      "Epoch:  30   =====> Loss= 0.140592536\n",
      "INFO:tensorflow:Restoring parameters from models/gru.ckpt\n",
      "Accuracy: 0.8875\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(123)\n",
    "model_path = \"models/gru.ckpt\"\n",
    "# tf Graph Input:  sentiment analysis data\n",
    "x = tf.placeholder(tf.float32, [None, max_l, 300], name='InputData')\n",
    "# masks\n",
    "m = tf.placeholder(tf.float32, [None, max_l, 1], name='MaskData')\n",
    "# Positive (1) or Negative (0) labels\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='LabelData')\n",
    "\n",
    "gru = GRU(300, hidden_states)\n",
    "\n",
    "gru_output = utils.process_sequence(gru, x, m)\n",
    "\n",
    "W = tf.Variable(tf.zeros([hidden_states, 1]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([1]), name='Bias')\n",
    "pred = tf.nn.sigmoid(tf.matmul(gru_output, W) + b)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)) - (1.-y)*tf.log(tf.clip_by_value((1.-pred), epsilon, 1.0)))\n",
    "\n",
    "with tf.name_scope('Adam'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    pred_tmp = tf.stack([pred, 1.-pred])\n",
    "    y_tmp = tf.stack([y, 1.-y])\n",
    "    acc = tf.equal(tf.argmax(pred_tmp, 0), tf.argmax(y_tmp, 0))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    print(\"Training started\")\n",
    "    best_val_acc = 0.\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(train)/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ms, batch_ys = data.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c = sess.run([optimizer, cost],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys, m: batch_ms})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        val_xs, val_ms, val_ys = data.val_batch()\n",
    "        val_acc = acc.eval({x: val_xs, m: val_ms, y: val_ys})\n",
    "        print(\"Accuracy on validation:\", val_acc)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = saver.save(sess, model_path)\n",
    "            print(\"        Model saved in file: %s\" % save_path)\n",
    "        print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    saver.restore(sess, model_path)\n",
    "    test_xs, test_ms, test_ys = data.test_batch()\n",
    "    print(\"Accuracy:\", acc.eval({x: test_xs, m: test_ms, y: test_ys}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 4</b> - comment on your findings:<br>\n",
    "We get better results. This is due to the fact that GRUs can model long term sequences that RNN can't. This helps in sentiment analysis as the keyword indicating the sentiment could be located anywhere in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Trying to Enhance Result    </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Accuracy on validation: 0.79583335\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  01   =====> Loss= 0.623749432\n",
      "Accuracy on validation: 0.8125\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  02   =====> Loss= 0.489201207\n",
      "Accuracy on validation: 0.8541667\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  03   =====> Loss= 0.347546843\n",
      "Accuracy on validation: 0.8625\n",
      "        Model saved in file: models/gru.ckpt\n",
      "Epoch:  04   =====> Loss= 0.291449620\n",
      "Accuracy on validation: 0.85833335\n",
      "Epoch:  05   =====> Loss= 0.248982789\n",
      "Accuracy on validation: 0.8333333\n",
      "Epoch:  06   =====> Loss= 0.207618582\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  07   =====> Loss= 0.159163137\n",
      "Accuracy on validation: 0.8333333\n",
      "Epoch:  08   =====> Loss= 0.137935071\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  09   =====> Loss= 0.116813356\n",
      "Accuracy on validation: 0.82916665\n",
      "Epoch:  10   =====> Loss= 0.078956968\n",
      "Accuracy on validation: 0.825\n",
      "Epoch:  11   =====> Loss= 0.061228380\n",
      "Accuracy on validation: 0.8375\n",
      "Epoch:  12   =====> Loss= 0.049682761\n",
      "Accuracy on validation: 0.825\n",
      "Epoch:  13   =====> Loss= 0.040159231\n",
      "Accuracy on validation: 0.84166664\n",
      "Epoch:  14   =====> Loss= 0.024806270\n",
      "Accuracy on validation: 0.825\n",
      "Epoch:  15   =====> Loss= 0.013559349\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  16   =====> Loss= 0.007670041\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  17   =====> Loss= 0.004907627\n",
      "Accuracy on validation: 0.8125\n",
      "Epoch:  18   =====> Loss= 0.003267271\n",
      "Accuracy on validation: 0.8125\n",
      "Epoch:  19   =====> Loss= 0.002558551\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  20   =====> Loss= 0.002107067\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  21   =====> Loss= 0.001954971\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  22   =====> Loss= 0.001794912\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  23   =====> Loss= 0.001659195\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  24   =====> Loss= 0.001526650\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  25   =====> Loss= 0.001405690\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  26   =====> Loss= 0.001335640\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  27   =====> Loss= 0.001253062\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  28   =====> Loss= 0.001201955\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  29   =====> Loss= 0.001139806\n",
      "Accuracy on validation: 0.81666666\n",
      "Epoch:  30   =====> Loss= 0.001079650\n",
      "INFO:tensorflow:Restoring parameters from models/gru.ckpt\n",
      "Accuracy: 0.8833333\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(123)\n",
    "model_path = \"models/gru.ckpt\"\n",
    "# tf Graph Input:  sentiment analysis data\n",
    "x = tf.placeholder(tf.float32, [None, max_l, 300], name='InputData')\n",
    "# masks\n",
    "m = tf.placeholder(tf.float32, [None, max_l, 1], name='MaskData')\n",
    "# Positive (1) or Negative (0) labels\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='LabelData')\n",
    "\n",
    "gru = GRU(300, hidden_states)\n",
    "\n",
    "gru_output = utils.process_sequence(gru, x, m)\n",
    "\n",
    "W = tf.Variable(tf.zeros([hidden_states, 1]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([1]), name='Bias')\n",
    "pred = tf.nn.sigmoid(tf.matmul(gru_output, W) + b)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)) - (1.-y)*tf.log(tf.clip_by_value((1.-pred), epsilon, 1.0)))\n",
    "\n",
    "with tf.name_scope('Adam'):\n",
    "    # Gradient Descent\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.01\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    pred_tmp = tf.stack([pred, 1.-pred])\n",
    "    y_tmp = tf.stack([y, 1.-y])\n",
    "    acc = tf.equal(tf.argmax(pred_tmp, 0), tf.argmax(y_tmp, 0))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    print(\"Training started\")\n",
    "    best_val_acc = 0.\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(train)/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ms, batch_ys = data.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c = sess.run([optimizer, cost],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys, m: batch_ms})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        val_xs, val_ms, val_ys = data.val_batch()\n",
    "        val_acc = acc.eval({x: val_xs, m: val_ms, y: val_ys})\n",
    "        print(\"Accuracy on validation:\", val_acc)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = saver.save(sess, model_path)\n",
    "            print(\"        Model saved in file: %s\" % save_path)\n",
    "        print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    saver.restore(sess, model_path)\n",
    "    test_xs, test_ms, test_ys = data.test_batch()\n",
    "    print(\"Accuracy:\", acc.eval({x: test_xs, m: test_ms, y: test_ys}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
